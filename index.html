<!DOCTYPE html>
<html>
<head>
  <title>CS 445 Final Project</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/flatly/bootstrap.min.css">
  <link rel="stylesheet" href="css/twentytwenty.css" />

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
  <script src="js/jquery.event.move.js"></script>
  <script src="js/jquery.twentytwenty.js"></script>
  <script src="js/jquery.sliphover.min.js"></script>
  <script type="text/javascript" async
          src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</head>

<body>
<div class="container">
  <div class="page-header" id="banner">
    <div class="row">
      <div class="col-xs-12 text-center">
	    <h1>Mobile Camera Image Quality Enhancement with pix2pix</h1>
		<h3 class="lead">Xiaotian Le (xle2) & Sihao Chen (schen149)</h3>
        <h4>CS 445 Final Project, Fall 2017</h4>
      </div>
    </div>
  </div>

  <div class="row">
    <div class="col-sm-10 col-sm-offset-1 col-xs-12">
      <div class="panel panel-primary">
        <div class="panel-heading">
          <div class="panel-title">
            <h3>Introduction</h3>
          </div>
        </div>
        <div class="panel-body">
          <h4>Motivation</h4>
          <p>
            Since the first introduction of smartphones to the world, integrated mobile digital cameras have gained wide popularity. 
			Photography is more accessible than usual thanks to mobile digital cameras' advantage in compactness over many professional digital cameras. 
			However, due to their limitation in physical size, mobile digital cameras often delivers inferior performance 
			in image capturing compared to professional digital cameras that are larger in size. Compared to the high quality images captured by professional digital cameras, 
			images captured by mobile digital cameras suffers from information loss. We believe that there three major causes to this:
			(1) Limited dynamic range (2) Smaller sensors are more sensitive to electronic circuit noises (3) Diffusion blur from smaller aperture 
			<br>
          </p>
		  <p>
			Here we provide two example images we captured under similar conditions using a full-frame mirrorless camera and a compact form-factor camera. 
			We can see that the dynamic range of the first image is clearly higher than the second image. Also, sharpness of the first image from visual perception is higher than 
			the second image. If we look a little closer, the light on the ceiling causes significant diffusion blur in the second image.
          </p>
          <div class="row exif">
            <img src="img/a7r2.jpg" class="col-xs-12" title="Camera: SONY ILCE-7RM2<br>Lens: Sony FE 12-24mm f4 G<br>Aperture: f/8<br>Shutter speed: 1/8 s<br>ISO: 100" />
          </div>
          <div class="row exif">
            <img src="img/gopro.jpg" class="col-xs-12" title="Camera: GoPro HERO4 Silver<br>Aperture: f/2.8<br>Shutter speed: 1/30 s<br>ISO: 110" />
          </div>
		  <p>
			<br>
			 In this project, we investigate each of the three causes and attempt to use neural method to recover low quality images that suffers from each type of information loss. 
          </p>

          <hr>
          <h4>Tools Utilized</h4>
          <p>
            For this project, we use the pix2pix system proposed in <a href="https://arxiv.org/abs/1611.07004">Image-to-Image Translation with Conditional Adversarial Networks</a> 
			for translation bewteen a low quality image to its high quality variant. All images we used for training and validation purposes are captured by either Xiaotian or Sihao. 
			No external dataset or image were involved during our experiments. 
          </p>
		  <p>
		  Pictures used in this web page, if not referenced or indicated, also belong to either Xiaotian or Sihao.
		  </p>
		  <p>
		  We developed our code for dataset preparation in MatLab. No external libraries were used during the process.  
		  </p>
		  <p>
			We use the existing <a href="https://github.com/affinelayer/pix2pix-tensorflow">pix2pix-tensorflow</a> implementation of pix2pix in TensorFlow available on GitHub.
		  </p>
          <hr>
          <h4>Source Images</h4>
          <p>
            Landscape scenes often have a wide dynamic range.
            Therefore, we use as source images a total of 183 landscape images taken in Yellowstone National Park with Sony a7R II,
            which is capable of producing images with a fairly wide dynamic range.
            All images are then resized to \(256 \times 256\),
            so that model training doesn't take weeks to finish.
            153 of them are used as the training set, while the remaining 30 are used as the validation set.
          </p>

          <p>
            Here are some examples.
          </p>
          <div class="row">
            <img src="img/source/1.jpg" class="col-md-6 col-xs-12" />
            <img src="img/source/2.jpg" class="col-md-6 col-xs-12" />
          </div>
          <div class="row">
            <img src="img/source/3.jpg" class="col-md-6 col-xs-12" />
            <img src="img/source/4.jpg" class="col-md-6 col-xs-12" />
          </div>
        </div>
      </div>

      <div class="panel panel-primary">
        <div class="panel-heading">
          <div class="panel-title">
            <h3>HDR Recovery</h3>
          </div>
        </div>
        <div class="panel-body">
          <p>
            We first attempt to train a model to convert low DR camera images into higher DR camera images.
            To achieve that the model must not only learn the new tone curve,
            but also learn to fill the missing shadow and highlight details based on the low DR image and its own knowledge.
            After prolonged training and trials, we have a model performing fairly well at this task.
          </p>

          <hr>
          <h4>Dataset Generation</h4>
          <p>
            To get pairs of images with one from a high DR camera and another from a low DR camera,
            we attempt to convert images originally from the high DR camera (input images) to simulate the shots from the low DR one (output images).
            We noticed that camera response functions incorporates characteristics of the camera's DR,
            as cameras with lower DR will have a narrower function domain to hide highlight clipping and shadow noise.
            Meanwhile, irradiance values, ignoring highlight clipping and shadow noise, should be the same for both cameras.
            Therefore, it is natural to apply the inverse high DR camera response function to the input image to find the irradiance values,
            and then the low DR camera response function to get the output image.<br>
            $$Z_{low} = c(Z_{high}) = g_{low}^{-1}(g_{high}(Z_{high}))$$
            We use the method in Project 4 to find the inverse camera response functions \(g\).
          </p>

          <p>
            Here are the estimated inverse response functions for Sony a7R II and GoPro HERO4.<br>
            Note that the one of a7R II has a much larger range.
          </p>
          <div class="img-compare">
            <img src="img/gopro_response.svg" width="100%" />
            <img src="img/a7r2_response.svg" width="100%" />
          </div>

          <br>
          <p>
            We then use <code>pchip</code> interpolation on \(g_{low}\),
            and <code>fzero</code> on it to find its inverse function values.<br>
            Note that \(z_{low}\) clips to 0 or 255 for extreme \(z_{high}\) values,
            indicating that the low DR camera image loses information.
          </p>
          <div class="row">
            <img src="img/conversion.svg" class="col-xs-12" />
          </div>

          <br>
          <p>
            We apply \(c\) to the source images to get the image pairs for training.<br>
          </p>

          <p>
            Here is an example training image,
            with the high DR image on the left and the low DR image on the right.<br>
            Note that grass field falls to black and some part of the sky clips to pure white.
          </p>
          <div class="row">
            <img src="img/hdr_dataset_example.jpg" class="col-xs-12" />
          </div>

          <hr>
          <h4>Training</h4>
          <p>
            We use TensorBoard to monitor training statistics.
            We run the model on the validation set to manually inspect the results for every 20 epochs.
          </p>

          <hr>
          <h4>Validation Set Results</h4>
          <p>
            After 20 epochs, the model learns a tone curve to adjust to the target images.<br>
            After 100 epochs, it successfully learns to fill the missing details, with many natural details.
          </p>

          <p>
            Here are some example validation set results.
            Move mouse onto the images to check the types of them.
          </p>

          <br>
          <p>
            The 100-epoch model manages to fill clouds into the sky highlights.
            Though some of the clouds are not there in the target image,
            they certainly make sense.
            It also fills texture details into the vast areas of shadows.
          </p>
          <div class="row exif">
            <img src="img/hdr_results/66-inputs.png" class="col-md-6 col-xs-12" title="Input Low DR Image" />
            <img src="img/hdr_results/66-targets.png" class="col-md-6 col-xs-12" title="Target High DR Image" />
          </div>
          <div class="row exif">
            <img src="img/hdr_results/66-outputs_20.png" class="col-md-6 col-xs-12" title="20-Epoch Model On Validation" />
            <img src="img/hdr_results/66-outputs_100.png" class="col-md-6 col-xs-12" title="100-Epoch Model On Validation" />
          </div>
          <div class="row">
            <div class="col-md-6 col-xs-12">
              <div class="img-compare input-epoch-100">
                <img src="img/hdr_results/66-inputs.png" width="100%" />
                <img src="img/hdr_results/66-outputs_100.png" width="100%" />
              </div>
            </div>
            <div class="col-md-6 col-xs-12">
              <div class="img-compare epoch-100-target">
                <img src="img/hdr_results/66-outputs_100.png" width="100%" />
                <img src="img/hdr_results/66-targets.png" width="100%" />
              </div>
            </div>
          </div>

          <br>
          <br>
          <p>
            The 100-epoch model manages to fill tree textures into the tree shadows.
            It also fills some imaginary clouds into the sky highlights,
            which could have been there,
            while the 20-epoch model doesn't.
          </p>
          <div class="row exif">
            <img src="img/hdr_results/144-inputs.png" class="col-md-6 col-xs-12" title="Input Low DR Image" />
            <img src="img/hdr_results/144-targets.png" class="col-md-6 col-xs-12" title="Target High DR Image" />
          </div>
          <div class="row exif">
            <img src="img/hdr_results/144-outputs_20.png" class="col-md-6 col-xs-12" title="20-Epoch Model On Validation" />
            <img src="img/hdr_results/144-outputs_100.png" class="col-md-6 col-xs-12" title="100-Epoch Model On Validation" />
          </div>
          <div class="row">
            <div class="col-md-6 col-xs-12">
              <div class="img-compare input-epoch-100">
                <img src="img/hdr_results/144-inputs.png" width="100%" />
                <img src="img/hdr_results/144-outputs_100.png" width="100%" />
              </div>
            </div>
            <div class="col-md-6 col-xs-12">
              <div class="img-compare epoch-100-target">
                <img src="img/hdr_results/144-outputs_100.png" width="100%" />
                <img src="img/hdr_results/144-targets.png" width="100%" />
              </div>
            </div>
          </div>

          <br>
          <br>
          <p>
            The 100-epoch model learns to reduce the intensity of the pure white sun highlight,
            which the 20-epoch model fails.
            It also fills mountain and grass details into the shadows.
          </p>
          <div class="row exif">
            <img src="img/hdr_results/174-inputs.png" class="col-md-6 col-xs-12" title="Input Low DR Image" />
            <img src="img/hdr_results/174-targets.png" class="col-md-6 col-xs-12" title="Target High DR Image" />
          </div>
          <div class="row exif">
            <img src="img/hdr_results/174-outputs_20.png" class="col-md-6 col-xs-12" title="20-Epoch Model On Validation" />
            <img src="img/hdr_results/174-outputs_100.png" class="col-md-6 col-xs-12" title="100-Epoch Model On Validation" />
          </div>
          <div class="row">
            <div class="col-md-6 col-xs-12">
              <div class="img-compare input-epoch-100">
                <img src="img/hdr_results/174-inputs.png" width="100%" />
                <img src="img/hdr_results/174-outputs_100.png" width="100%" />
              </div>
            </div>
            <div class="col-md-6 col-xs-12">
              <div class="img-compare epoch-100-target">
                <img src="img/hdr_results/174-outputs_100.png" width="100%" />
                <img src="img/hdr_results/174-targets.png" width="100%" />
              </div>
            </div>
          </div>

          <br>
          <br>
          <p>
            The 100-epoch model manages to fill stone-like textures into the mountain and ground shadows.
            Though some of the textures might not be present in the target image, they look natural.
          </p>
          <div class="row exif">
            <img src="img/hdr_results/60-inputs.png" class="col-md-6 col-xs-12" title="Input Low DR Image" />
            <img src="img/hdr_results/60-targets.png" class="col-md-6 col-xs-12" title="Target High DR Image" />
          </div>
          <div class="row exif">
            <img src="img/hdr_results/60-outputs_20.png" class="col-md-6 col-xs-12" title="20-Epoch Model On Validation" />
            <img src="img/hdr_results/60-outputs_100.png" class="col-md-6 col-xs-12" title="100-Epoch Model On Validation" />
          </div>
          <div class="row">
            <div class="col-md-6 col-xs-12">
              <div class="img-compare input-epoch-100">
                <img src="img/hdr_results/60-inputs.png" width="100%" />
                <img src="img/hdr_results/60-outputs_100.png" width="100%" />
              </div>
            </div>
            <div class="col-md-6 col-xs-12">
              <div class="img-compare epoch-100-target">
                <img src="img/hdr_results/60-outputs_100.png" width="100%" />
                <img src="img/hdr_results/60-targets.png" width="100%" />
              </div>
            </div>
          </div>
        </div>
      </div>

      <div class="panel panel-primary">
        <div class="panel-heading">
          <div class="panel-title">
            <h3>Noise Reduction</h3>
          </div>
        </div>
        <div class="panel-body">
          <h4>Dataset Generation</h4>
          <p>
            To simulate the electronic circuit noise of images captured under high ISO settings, we first generate a zero-mean Gaussian white noise image with fixed 
			variance at every pixel. To mimic the process of demosaicing interpolation of bayer filter, we designed 
			the filter shown in the figure below based on the observation that the value of a given pixel is most correlated 
			with (1) itself; (2) its four adjacent pixels. 
          </p>
          <div class="row">
			<img src="img/bayer.png" class="col-md-6 col-xs-12" />
            <img src="img/noise_sim_filter.png" class="col-md-4 col-xs-12" />
          </div>
		  <em>Anon. 2017. Bayer filter. (November 2017). Retrieved December 14, 2017 from <a href="https://en.wikipedia.org/wiki/Bayer_filter">https://en.wikipedia.org/wiki/Bayer_filter</a></em>
		  <p>
			<br>
		    We apply this filter to the noise image we generated and add the result to the original image to obtain our simulated noisy image. 
			<br>
			Here we show the comparson between the original image (Before) and our simulated result (After)
		  </p>
          <div class="img-compare">
            <img src="img/orig.jpg" width="100%" />
            <img src="img/noise_gen.jpg" width="100%" />
          </div>
		  <p>
		    <br>
		    Comparson between high ISO image (Before) and our simulated result (After)
		  </p>
          <div class="img-compare">
            <img src="img/noise_orig.jpg" width="100%" />
            <img src="img/noise_gen.jpg" width="100%" />
          </div>
          <hr>
          <h4>Training</h4>
          <p>
            ...
          </p>

          <hr>
          <h4>Results</h4>
          <p>
            ...
          </p>
        </div>
      </div>

      <div class="panel panel-primary">
        <div class="panel-heading">
          <div class="panel-title">
            <h3>Diffraction Blur Reduction</h3>
          </div>
        </div>
        <div class="panel-body">
          <h4>Dataset Generation</h4>
          <p>
            ...
          </p>

          <hr>
          <h4>Training</h4>
          <p>
            We obtain the most satisfactory results after 120 epochs of training.
          </p>

          <hr>
          <h4>Validation Set Results</h4>
          <div class="row">
            <div class="col-md-6 col-xs-12">
              <div class="img-compare input-output">
                <img src="img/deblur_results/48-inputs.png" width="100%" />
                <img src="img/deblur_results/48-outputs.png" width="100%" />
              </div>
            </div>
            <div class="col-md-6 col-xs-12">
              <div class="img-compare output-target">
                <img src="img/deblur_results/48-outputs.png" width="100%" />
                <img src="img/deblur_results/48-targets.png" width="100%" />
              </div>
            </div>
          </div>

          <br>
          <br>
          <div class="row">
            <div class="col-md-6 col-xs-12">
              <div class="img-compare input-output">
                <img src="img/deblur_results/78-inputs.png" width="100%" />
                <img src="img/deblur_results/78-outputs.png" width="100%" />
              </div>
            </div>
            <div class="col-md-6 col-xs-12">
              <div class="img-compare output-target">
                <img src="img/deblur_results/78-outputs.png" width="100%" />
                <img src="img/deblur_results/78-targets.png" width="100%" />
              </div>
            </div>
          </div>

          <br>
          <br>
          <div class="row">
            <div class="col-md-6 col-xs-12">
              <div class="img-compare input-output">
                <img src="img/deblur_results/84-inputs.png" width="100%" />
                <img src="img/deblur_results/84-outputs.png" width="100%" />
              </div>
            </div>
            <div class="col-md-6 col-xs-12">
              <div class="img-compare output-target">
                <img src="img/deblur_results/84-outputs.png" width="100%" />
                <img src="img/deblur_results/84-targets.png" width="100%" />
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<script>
    $(window).on('load', function() {
        $(".img-compare").twentytwenty({
            default_offset_pct: 0.35
        });
        $(".img-compare.input-epoch-100").twentytwenty({
            before_label: 'Input Image',
            after_label: '100 Epochs',
            default_offset_pct: 0.35
        });
        $(".img-compare.epoch-100-target").twentytwenty({
            before_label: '100 Epochs',
            after_label: 'Target Image',
            default_offset_pct: 0.65
        });
        $(".img-compare.input-output").twentytwenty({
            before_label: 'Input Image',
            after_label: 'Output Image',
            default_offset_pct: 0.35
        });
        $(".img-compare.output-target").twentytwenty({
            before_label: 'Output Image',
            after_label: 'Target Image',
            default_offset_pct: 0.65
        });
        $(".exif").sliphover({
            backgroundColor: 'rgba(0,0,0,0)',
            height: '50%'
        });
    });
</script>
</body>
</html>
